{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPZZjRCYOIUuyljO2IbkVPk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Google Drive 마운트 & 경로 설정\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, glob, shutil\n","\n","OUT_DIR = \"/content/outputs\"                       # 로컬 결과/체크포인트 폴더\n","DRIVE_DIR = \"/content/drive/MyDrive/dcgan_checkpoints\"  # 드라이브 백업 폴더\n","os.makedirs(OUT_DIR, exist_ok=True)\n","os.makedirs(DRIVE_DIR, exist_ok=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P5KTo_lJ4UjB","executionInfo":{"status":"ok","timestamp":1755952238613,"user_tz":-540,"elapsed":25186,"user":{"displayName":"고현서","userId":"14122908031781224765"}},"outputId":"535034bf-878e-400f-c242-8a61986fe195"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### 데이터 불러오기"],"metadata":{"id":"OC7F1WfbaQep"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":77},"id":"BHaEVif1YItM","executionInfo":{"status":"ok","timestamp":1755952252803,"user_tz":-540,"elapsed":11437,"user":{"displayName":"고현서","userId":"14122908031781224765"}},"outputId":"16730a2d-4200-4a32-c7be-524fd869b1c4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-6a9e4fe6-5ac5-4ea5-a9bc-3bb65d2167c6\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-6a9e4fe6-5ac5-4ea5-a9bc-3bb65d2167c6\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n"]}],"source":["from google.colab import files\n","files.upload()  # kaggle.json 업로드 창 표시\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","source":["#!/bin/bash\n","!kaggle datasets download deewakarchakraborty/portrait-paintings"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-iRFiz-YUVC","executionInfo":{"status":"ok","timestamp":1755952263198,"user_tz":-540,"elapsed":8741,"user":{"displayName":"고현서","userId":"14122908031781224765"}},"outputId":"d829045c-83be-4e34-c20a-0bed8996ba37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/deewakarchakraborty/portrait-paintings\n","License(s): CC0-1.0\n","Downloading portrait-paintings.zip to /content\n"," 99% 222M/223M [00:00<00:00, 710MB/s] \n","100% 223M/223M [00:00<00:00, 749MB/s]\n"]}]},{"cell_type":"markdown","source":["### 기본 설정"],"metadata":{"id":"xBmo5L3_aNvk"}},{"cell_type":"code","source":["!unzip -q portrait-paintings.zip"],"metadata":{"id":"QV2DMLqwYgxX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, glob, random, math, time\n","from PIL import Image\n","import numpy as np\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision\n","from torchvision import transforms\n","from torchvision.utils import save_image, make_grid"],"metadata":{"id":"fUlkhNk2YnzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"DEVICE:\", DEVICE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HGlHom8SaBc0","executionInfo":{"status":"ok","timestamp":1755952282152,"user_tz":-540,"elapsed":35,"user":{"displayName":"고현서","userId":"14122908031781224765"}},"outputId":"68faad31-cab0-4d0b-9c2f-b042134f11aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DEVICE: cuda\n"]}]},{"cell_type":"code","source":["# 재현성\n","SEED = 2025\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED);\n","if DEVICE.type == \"cuda\":\n","    torch.cuda.manual_seed_all(SEED)"],"metadata":{"id":"8TixfdyuaGY1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 하이퍼파라미터\n","image_size = 64        # DCGAN 기본 사이즈(빠름). 더 선명하게는 128도 가능\n","nc = 3                 # 채널 수(RGB=3)\n","nz = 100               # 잠재 벡터 차원\n","ngf = 64               # G의 feature map 크기\n","ndf = 64               # D의 feature map 크기\n","batch_size = 64\n","num_epochs = 100       # 결과 보고 늘리기\n","lr = 0.0002\n","beta1 = 0.5            # Adam beta1(DCGAN 권장값)"],"metadata":{"id":"zFeCFyWgaIpz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 데이터 준비"],"metadata":{"id":"qMupd4CUaJ4B"}},{"cell_type":"code","source":["class PortraitFolder(Dataset):\n","    def __init__(self, root_dir, transform=None, exts=(\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.webp\",\"*.bmp\")):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        files = []\n","        # 지정된 확장자별로 파일 경로 수집 (현재 폴더 + 하위 폴더까지 검색)\n","        for ext in exts:\n","            files.extend(glob.glob(os.path.join(root_dir, ext)))  # 루트 폴더 1단계\n","            files.extend(glob.glob(os.path.join(root_dir, \"**\", ext), recursive=True)) # 재귀적으로 하위 폴더까지\n","        # 중복 제거 + 정렬\n","        self.files = sorted(list(set(files)))\n","\n","        # 파일이 정말 이미지인지 간단 필터링(깨진 파일 방지)\n","        ok_files = []\n","        for f in self.files:\n","            try:\n","                with Image.open(f) as im:\n","                    im.verify() # 이미지 헤더만 검사 (실제 로딩은 안 함)\n","                ok_files.append(f)\n","            except Exception:\n","                pass # 열리지 않거나 깨진 파일은 무시\n","        self.files = ok_files\n","\n","        # 유효한 이미지가 하나도 없으면 에러 발생\n","        if len(self.files) == 0:\n","            raise RuntimeError(f\"No images found under: {root_dir}\")\n","\n","        print(f\"Found {len(self.files)} images.\") # 최종 이미지 개수 출력\n","\n","    def __len__(self):\n","        return len(self.files) # 전체 이미지 개수 반환\n","\n","    def __getitem__(self, idx):\n","        # index에 해당하는 이미지 파일 로드\n","        path = self.files[idx]\n","        img = Image.open(path).convert(\"RGB\") # RGB로 통일 (흑백/투명 채널 제거)\n","        if self.transform:\n","            img = self.transform(img) # 전처리(transform) 적용\n","        return img\n","\n","# 전처리\n","tfm = transforms.Compose([\n","    transforms.Resize(image_size), # 짧은 변을 image_size로 리사이즈\n","    transforms.CenterCrop(image_size), # 중앙에서 image_size x image_size 크기로 잘라내기\n","    transforms.ToTensor(), # [0,255] → [0,1] 범위의 Tensor로 변환\n","    transforms.Normalize([0.5]*3, [0.5]*3),  # 평균 0.5, 표준편차 0.5로 정규화 → [-1,1] 범위\n","])\n","\n","# Dataset / DataLoader 생성\n","ds = PortraitFolder(\"/content/Images\", transform=tfm) # 이미지 폴더 Dataset 생성\n","dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True if DEVICE.type==\"cuda\" else False, drop_last=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S-cGOHFjacew","executionInfo":{"status":"ok","timestamp":1755952298881,"user_tz":-540,"elapsed":500,"user":{"displayName":"고현서","userId":"14122908031781224765"}},"outputId":"08cb384b-f47a-4080-ffb6-1608a6c244f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5734 images.\n"]}]},{"cell_type":"markdown","source":["### DCGAN 모델 정의\n","- Generator: ConvTranspose2d → BN → ReLU 반복, 마지막 Tanh\n","- Discriminator: Conv2d → (BN) → LeakyReLU 반복, 마지막엔 로짓 1개"],"metadata":{"id":"olHHUVUIamdj"}},{"cell_type":"code","source":["# 가중치 초기화(DCGAN 권장)\n","def weights_init_dcgan(m):\n","    classname = m.__class__.__name__.lower() # 모듈의 클래스 이름 확인 (예: Conv2d, BatchNorm2d 등)\n","\n","    # 1) Convolution 계층 초기화\n","    if \"conv\" in classname:\n","        # Conv 계층의 weight를 평균 0, 표준편차 0.02인 정규분포로 초기화\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","        # bias가 있다면 0으로 초기화\n","        if getattr(m, \"bias\", None) is not None and m.bias is not None:\n","            nn.init.zeros_(m.bias.data)\n","\n","    # 2) BatchNorm 계층 초기화\n","    elif \"batchnorm\" in classname:\n","        # weight(γ)를 평균 1.0, 표준편차 0.02인 정규분포로 초기화 → 스케일 파라미터\n","        if getattr(m, \"weight\", None) is not None:\n","            nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        # bias(β)는 0으로 초기화 → 이동 파라미터\n","        if getattr(m, \"bias\", None) is not None:\n","            nn.init.zeros_(m.bias.data)"],"metadata":{"id":"sx_kGduTawBv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generator\n","class Generator(nn.Module):\n","    def __init__(self, nz=100, ngf=64, nc=3):\n","        super().__init__()\n","        self.main = nn.Sequential(\n","            # 입력: (nz, 1, 1) → 첫 입력은 랜덤 노이즈 z (100차원 벡터)\n","            nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf*8),\n","            nn.ReLU(True),\n","            # upsampling: (ngf*8, 4, 4) → (ngf*4, 8, 8)\n","            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf*4),\n","            nn.ReLU(True),\n","            # (ngf*4, 8, 8) → (ngf*2, 16, 16)\n","            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf*2),\n","            nn.ReLU(True),\n","            # (ngf*2, 16, 16) → (ngf, 32, 32)\n","            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf),\n","            nn.ReLU(True),\n","            # (ngf, 32, 32) → (nc=3, 64, 64) 최종 RGB 이미지\n","            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n","            nn.Tanh(),  # 출력 범위 [-1, 1]\n","        )\n","    def forward(self, z):\n","        return self.main(z) # (B, 3, 64, 64)"],"metadata":{"id":"1ZYnWs7ta2Rc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Discriminator\n","class Discriminator(nn.Module):\n","    def __init__(self, nc=3, ndf=64):\n","        super().__init__()\n","        self.main = nn.Sequential(\n","            # 입력: (3, 64, 64) → 첫 레이어에서 특징 추출\n","            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # (ndf, 32, 32) → (ndf*2, 16, 16)\n","            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf*2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # (ndf*2, 16, 16) → (ndf*4, 8, 8)\n","            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf*4),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # (ndf*4, 8, 8) → (ndf*8, 4, 4)\n","            nn.Conv2d(ndf*4, ndf*8, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf*8),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # (ndf*8, 4, 4) → (1, 1, 1) → 진짜/가짜 판별 스칼라 값\n","            nn.Conv2d(ndf*8, 1, 4, 1, 0, bias=False),\n","            # Sigmoid 없음 (PyTorch에서 BCEWithLogitsLoss가 자동으로 Sigmoid 내장)\n","        )\n","    def forward(self, x):\n","        out = self.main(x)        # 출력: (B, 1, 1, 1)\n","        return out.view(-1)       # reshape → (B,)"],"metadata":{"id":"F96UPIi3a5XY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 생성 + 초기화\n","netG = Generator(nz, ngf, nc).to(DEVICE)\n","netD = Discriminator(nc, ndf).to(DEVICE)\n","netG.apply(weights_init_dcgan)\n","netD.apply(weights_init_dcgan)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xnpLen65a7wU","executionInfo":{"status":"ok","timestamp":1755952307937,"user_tz":-540,"elapsed":344,"user":{"displayName":"고현서","userId":"14122908031781224765"}},"outputId":"0ee030c0-c834-45ac-da86-88460ea6dea2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Discriminator(\n","  (main): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","  )\n",")"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["### 손실 함수 & 옵티마이저 설정"],"metadata":{"id":"DM1uPcMlmj6U"}},{"cell_type":"code","source":["# 손실/옵티마이저\n","criterion = nn.BCEWithLogitsLoss().to(DEVICE)\n","optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n","optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"],"metadata":{"id":"YfzYK2Msa9Q-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 고정 노이즈(진행 모니터)\n","fixed_noise = torch.randn(64, nz, 1, 1, device=DEVICE)"],"metadata":{"id":"AMkc0k7aa_k8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 학습 루프\n","\n","- D는 real(=1)과 fake(=0)를 구분하도록 학습\n","- G는 D를 속이도록(=1로 분류되도록) 학습\n","- 매 epoch마다 샘플 이미지 저장"],"metadata":{"id":"U64uMpFNbAPE"}},{"cell_type":"code","source":["# 학습\n","iters = 0 # 전체 학습 step 수를 기록할 카운터\n","log_interval = 100 # 로그 출력 주기(몇 iteration마다 결과 보여줄지)\n","os.makedirs(OUT_DIR, exist_ok=True) # 출력 폴더 생성 (없으면 새로 만듦)\n","\n","for epoch in range(1, num_epochs+1):\n","    netG.train(); netD.train() # 학습 모드로 전환 (BN/Dropout 동작 차이 있음)\n","    pbar = tqdm(dl, desc=f\"[Epoch {epoch}/{num_epochs}]\") # 진행 상태 표시\n","    for real in pbar:\n","        # (1) Discriminator 학습\n","        real = real.to(DEVICE) # 진짜 이미지 배치 (GPU로)\n","        bsz = real.size(0) # 현재 배치 크기\n","        real_label = torch.ones(bsz, device=DEVICE) # 진짜 라벨 = 1\n","        fake_label = torch.zeros(bsz, device=DEVICE) # 가짜 라벨 = 0\n","\n","        optimizerD.zero_grad(set_to_none=True) # D의 기울기 초기화\n","        d_real = netD(real) # 진짜 이미지 판별\n","        d_real_loss = criterion(d_real, real_label) # 진짜 → 1이 되도록 BCE Loss\n","\n","        noise = torch.randn(bsz, nz, 1, 1, device=DEVICE) # 랜덤 노이즈\n","        fake = netG(noise).detach() # Generator로 가짜 이미지 생성 (detach → G 업데이트 X)\n","        d_fake = netD(fake) # D가 가짜 이미지 판별\n","        d_fake_loss = criterion(d_fake, fake_label) # 가짜 → 0이 되도록 BCE Loss\n","\n","        d_loss = d_real_loss + d_fake_loss # D의 최종 손실 (진짜+가짜)\n","        d_loss.backward() # 역전파\n","        optimizerD.step() # D의 파라미터 업데이트\n","\n","        # (2) Generator 학습\n","        optimizerG.zero_grad(set_to_none=True) # G의 기울기 초기화\n","        noise = torch.randn(bsz, nz, 1, 1, device=DEVICE) # 새로운 노이즈\n","        gen = netG(noise) # G로 가짜 이미지 생성\n","        d_gen = netD(gen) # D가 그 이미지를 판별\n","        g_loss = criterion(d_gen, real_label) # G는 D를 속여서 \"진짜=1\"로 만들고 싶음\n","        g_loss.backward() # 역전파\n","        optimizerG.step() # G의 파라미터 업데이트\n","\n","        # (3) 로그 출력\n","        iters += 1\n","        if iters % log_interval == 0:\n","            pbar.set_postfix({\n","                \"D_real\": f\"{d_real_loss.item():.3f}\",\n","                \"D_fake\": f\"{d_fake_loss.item():.3f}\",\n","                \"D\": f\"{d_loss.item():.3f}\",\n","                \"G\": f\"{g_loss.item():.3f}\"\n","            })\n","\n","    # (4) 에폭마다 샘플 이미지 저장\n","    netG.eval() # 평가 모드 (BN/Dropout 고정)\n","    with torch.no_grad():\n","        fakes = netG(fixed_noise).cpu() # 고정 노이즈 입력으로 가짜 이미지 생성\n","        save_image((fakes*0.5+0.5).clamp(0,1), # [-1,1] → [0,1] 범위로 복원\n","                   fp=os.path.join(OUT_DIR, f\"samples_epoch_{epoch:03d}.png\"),\n","                   nrow=8) # 8x8 그리드로 저장\n","\n","    # 체크포인트 저장\n","    torch.save({\n","        \"epoch\": epoch,\n","        \"netG\": netG.state_dict(), # Generator 가중치\n","        \"netD\": netD.state_dict(), # Discriminator 가중치\n","        \"optimizerG\": optimizerG.state_dict(), # G 옵티마이저 상태\n","        \"optimizerD\": optimizerD.state_dict(), # D 옵티마이저 상태\n","        \"nz\": nz, \"ngf\": ngf, \"ndf\": ndf, \"nc\": nc, \"image_size\": image_size,\n","    }, os.path.join(OUT_DIR, f\"ckpt_{epoch:03d}.pt\"))\n","\n","print(\"학습 완료! 샘플 이미지는\", OUT_DIR, \"에 저장되었습니다.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"_nh7zcE_bHK6","executionInfo":{"status":"ok","timestamp":1755952503057,"user_tz":-540,"elapsed":185191,"user":{"displayName":"고현서","userId":"14122908031781224765"}},"outputId":"4e2547ab-be1c-4d97-cd10-4d7b6a0dbcb8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[Epoch 1/10]: 100%|██████████| 89/89 [00:21<00:00,  4.21it/s]\n","[Epoch 2/10]: 100%|██████████| 89/89 [00:17<00:00,  5.11it/s, D_real=0.126, D_fake=0.255, D=0.382, G=4.471]\n","[Epoch 3/10]: 100%|██████████| 89/89 [00:17<00:00,  4.95it/s, D_real=0.140, D_fake=0.072, D=0.211, G=3.446]\n","[Epoch 4/10]: 100%|██████████| 89/89 [00:19<00:00,  4.53it/s, D_real=0.278, D_fake=0.008, D=0.285, G=7.805]\n","[Epoch 5/10]: 100%|██████████| 89/89 [00:17<00:00,  5.11it/s, D_real=0.018, D_fake=0.138, D=0.157, G=8.876]\n","[Epoch 6/10]: 100%|██████████| 89/89 [00:17<00:00,  5.06it/s, D_real=0.431, D_fake=0.452, D=0.883, G=2.537]\n","[Epoch 7/10]: 100%|██████████| 89/89 [00:17<00:00,  5.00it/s, D_real=0.378, D_fake=0.202, D=0.579, G=4.797]\n","[Epoch 8/10]: 100%|██████████| 89/89 [00:18<00:00,  4.80it/s, D_real=0.120, D_fake=0.487, D=0.608, G=7.910]\n","[Epoch 9/10]: 100%|██████████| 89/89 [00:17<00:00,  5.04it/s, D_real=0.732, D_fake=0.181, D=0.914, G=3.994]\n","[Epoch 10/10]: 100%|██████████| 89/89 [00:17<00:00,  5.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["학습 완료! 샘플 이미지는 /content/outputs 에 저장되었습니다.\n"]}]},{"cell_type":"markdown","source":["### 최신 체크포인트를 드라이브로 백업"],"metadata":{"id":"IpTVPK4k5gHY"}},{"cell_type":"code","source":["# 로컬 OUT_DIR의 가장 최신 ckpt를 드라이브로 복사\n","ckpts_local = sorted(glob.glob(os.path.join(OUT_DIR, \"ckpt_*.pt\")))\n","if not ckpts_local:\n","    raise RuntimeError(\"OUT_DIR에 ckpt_*.pt가 없습니다. 먼저 학습을 실행하세요.\")\n","last_ckpt_local = ckpts_local[-1]\n","dst = os.path.join(DRIVE_DIR, os.path.basename(last_ckpt_local))\n","shutil.copy2(last_ckpt_local, dst)\n","print(\"드라이브 백업 완료:\", dst)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u17U8t-e5Dal","executionInfo":{"status":"ok","timestamp":1755952538703,"user_tz":-540,"elapsed":180,"user":{"displayName":"고현서","userId":"14122908031781224765"}},"outputId":"4656f432-81bd-4042-9cec-413844233022"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["드라이브 백업 완료: /content/drive/MyDrive/dcgan_checkpoints/ckpt_010.pt\n"]}]},{"cell_type":"markdown","source":["### 드라이브에서 체크포인트 불러와 이어 학습"],"metadata":{"id":"oct2g52K5adj"}},{"cell_type":"code","source":["# 드라이브 최신 ckpt 로드 & 이어 학습\n","import torch, glob, os\n","from tqdm import tqdm\n","\n","# 1) 최신 ckpt 선택\n","ckpts_on_drive = sorted(glob.glob(os.path.join(DRIVE_DIR, \"ckpt_*.pt\")))\n","if not ckpts_on_drive:\n","    raise RuntimeError(\"DRIVE_DIR에 ckpt_*.pt가 없습니다.\")\n","ckpt_path = ckpts_on_drive[-1]\n","print(\"불러올 체크포인트:\", ckpt_path)\n","\n","# 2) 로드 & 복구 (모델/옵티마이저는 기존 정의 사용)\n","ckpt = torch.load(ckpt_path, map_location=DEVICE)\n","\n","# (모델 구조 하이퍼파라미터 동기화 — 혹시 달라졌다면 아래 값으로 덮어쓰기)\n","nz  = ckpt.get(\"nz\", nz)\n","ngf = ckpt.get(\"ngf\", ngf)\n","ndf = ckpt.get(\"ndf\", ndf)\n","nc  = ckpt.get(\"nc\", nc)\n","image_size = ckpt.get(\"image_size\", image_size)\n","\n","# 모델/옵티마이저 state 로드\n","netG.load_state_dict(ckpt[\"netG\"])\n","netD.load_state_dict(ckpt[\"netD\"])\n","if \"optimizerG\" in ckpt and \"optimizerD\" in ckpt:\n","    optimizerG.load_state_dict(ckpt[\"optimizerG\"])\n","    optimizerD.load_state_dict(ckpt[\"optimizerD\"])\n","    print(\"옵티마이저까지 복원 완료\")\n","else:\n","    print(\"옵티마이저 상태 없음 → 옵티마이저는 새로 시작\")\n","\n","resume_from_epoch = ckpt.get(\"epoch\", 0)\n","print(f\"{resume_from_epoch} 에폭 이후부터 이어서 학습합니다.\")\n","\n","# 3) 이어서 k에폭만큼 더 학습\n","k = 10  # 추가 학습할 에폭 수 (원하는 숫자로 변경)\n","start_ep = resume_from_epoch + 1\n","end_ep   = resume_from_epoch + k\n","\n","for epoch in range(start_ep, end_ep+1):\n","    netG.train(); netD.train()\n","    pbar = tqdm(dl, desc=f\"[Resume Epoch {epoch}]\")\n","    for real in pbar:\n","        real = real.to(DEVICE)\n","        bsz = real.size(0)\n","        real_label = torch.ones(bsz, device=DEVICE)\n","        fake_label = torch.zeros(bsz, device=DEVICE)\n","\n","        # D 업데이트\n","        optimizerD.zero_grad(set_to_none=True)\n","        d_real = netD(real)\n","        d_real_loss = criterion(d_real, real_label)\n","        noise = torch.randn(bsz, nz, 1, 1, device=DEVICE)\n","        fake = netG(noise).detach()\n","        d_fake = netD(fake)\n","        d_fake_loss = criterion(d_fake, fake_label)\n","        d_loss = d_real_loss + d_fake_loss\n","        d_loss.backward()\n","        optimizerD.step()\n","\n","        # G 업데이트\n","        optimizerG.zero_grad(set_to_none=True)\n","        noise = torch.randn(bsz, nz, 1, 1, device=DEVICE)\n","        gen = netG(noise)\n","        d_gen = netD(gen)\n","        g_loss = criterion(d_gen, real_label)\n","        g_loss.backward()\n","        optimizerG.step()\n","\n","    # 샘플/체크포인트 저장 (에폭 번호를 이어서 저장)\n","    netG.eval()\n","    with torch.no_grad():\n","        fakes = netG(fixed_noise).cpu()\n","        save_image((fakes*0.5+0.5).clamp(0,1),\n","                   fp=os.path.join(OUT_DIR, f\"samples_epoch_{epoch:03d}.png\"),\n","                   nrow=8)\n","\n","    torch.save({\n","        \"epoch\": epoch,\n","        \"netG\": netG.state_dict(),\n","        \"netD\": netD.state_dict(),\n","        \"optimizerG\": optimizerG.state_dict(),\n","        \"optimizerD\": optimizerD.state_dict(),\n","        \"nz\": nz, \"ngf\": ngf, \"ndf\": ndf, \"nc\": nc, \"image_size\": image_size,\n","    }, os.path.join(OUT_DIR, f\"ckpt_{epoch:03d}.pt\"))\n","\n","print(\"Resume 학습 완료!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"UQyufQJK5O4A","executionInfo":{"status":"error","timestamp":1755952596877,"user_tz":-540,"elapsed":11645,"user":{"displayName":"고현서","userId":"14122908031781224765"}},"outputId":"8a14f34e-e0cf-415b-e95f-c06a5f40af0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["불러올 체크포인트: /content/drive/MyDrive/dcgan_checkpoints/ckpt_010.pt\n","옵티마이저까지 복원 완료\n","10 에폭 이후부터 이어서 학습합니다.\n"]},{"output_type":"stream","name":"stderr","text":["[Resume Epoch 11]:  56%|█████▌    | 50/89 [00:11<00:08,  4.41it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2820056562.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"[Resume Epoch {epoch}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mreal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mbsz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["### 마지막 체크포인트만 드라이브에 다시 백업"],"metadata":{"id":"OxgOxiwQ5VW_"}},{"cell_type":"code","source":["# 가장 최신 로컬 ckpt를 드라이브에 복사\n","ckpts_local = sorted(glob.glob(os.path.join(OUT_DIR, \"ckpt_*.pt\")))\n","last_ckpt_local = ckpts_local[-1]\n","dst = os.path.join(DRIVE_DIR, os.path.basename(last_ckpt_local))\n","shutil.copy2(last_ckpt_local, dst)\n","print(\"드라이브 백업 완료:\", dst)"],"metadata":{"id":"hQRJoloW5SPf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GIF 만들기 (samples_epoch_XXX.png -> dcgan_training.gif)\n","import os, glob\n","from PIL import Image\n","from IPython.display import Image as DispImage, display\n","from google.colab import files\n","\n","out_dir = \"/content/outputs\"\n","gif_path = os.path.join(out_dir, \"dcgan_training.gif\")\n","\n","# 에폭 순서대로 정렬\n","frames = sorted(glob.glob(os.path.join(out_dir, \"samples_epoch_*.png\")))\n","\n","if len(frames) == 0:\n","    raise RuntimeError(\"outputs 폴더에 samples_epoch_*.png가 없어요. 학습 루프가 이미지를 저장했는지 확인하세요!\")\n","\n","# 첫 프레임 기준으로 GIF 저장\n","imgs = [Image.open(p).convert(\"RGB\") for p in frames]\n","imgs[0].save(\n","    gif_path,\n","    save_all=True,\n","    append_images=imgs[1:],\n","    duration=500,   # 프레임 간 시간(ms). 더 빠르게=작게, 느리게=크게\n","    loop=0          # 0이면 무한 반복\n",")\n","\n","print(f\"GIF 저장 완료: {gif_path}\")\n","display(DispImage(filename=gif_path))  # 노트북에 표시\n","files.download(gif_path)               # 로컬로 다운로드"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":565,"output_embedded_package_id":"1IchmQery_dVf1pCth5OfiOceUAg-kry6"},"id":"6a4knpJTbNJX","executionInfo":{"status":"ok","timestamp":1755946910542,"user_tz":-540,"elapsed":29083,"user":{"displayName":"고현서","userId":"14122908031781224765"}},"outputId":"6f2cd618-c72e-426a-a507-ae361fca8cee"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import cv2\n","import os, glob\n","from IPython.display import Video, display\n","from google.colab import files\n","\n","out_dir = \"/content/outputs\"\n","video_path = os.path.join(out_dir, \"dcgan_training.mp4\")\n","\n","# 에폭 이미지 경로 정렬\n","frames = sorted(glob.glob(os.path.join(out_dir, \"samples_epoch_*.png\")))\n","if len(frames) == 0:\n","    raise RuntimeError(\"outputs 폴더에 samples_epoch_*.png가 없어요. 먼저 학습 후 샘플 이미지를 확인하세요!\")\n","\n","# 첫 이미지 사이즈 가져오기\n","sample = cv2.imread(frames[0])\n","h, w, _ = sample.shape\n","\n","# 비디오 라이터 정의 (MP4, fps=2 → 초당 2장)\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","fps = 2  # 초당 프레임 수. 더 빠르게 보려면 4~8 정도로 조정 가능\n","video = cv2.VideoWriter(video_path, fourcc, fps, (w, h))\n","\n","# 모든 이미지 프레임 추가\n","for f in frames:\n","    img = cv2.imread(f)\n","    video.write(img)\n","\n","video.release()\n","print(f\"MP4 저장 완료: {video_path}\")\n","\n","# Colab에서 바로 재생\n","display(Video(video_path, embed=True))\n","\n","# 다운로드 링크\n","files.download(video_path)"],"metadata":{"id":"iFFeH8yvfDYJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### “실제 vs 생성” 별도 분류기"],"metadata":{"id":"clWtmCG7m_ds"}},{"cell_type":"markdown","source":["### 생성 이미지 뽑아두기"],"metadata":{"id":"nWDP2sDW74jR"}},{"cell_type":"code","source":["# 생성 이미지 대량 저장\n","gen_dir = \"/content/gen_images\"\n","os.makedirs(gen_dir, exist_ok=True)\n","\n","netG.eval() # Generator를 평가 모드로 (BN/Dropout 고정 → 안정적인 출력)\n","N = 2000  # 필요 수량 (예: 2000장) — Colab 시간에 맞춰 조절\n","bs = 64\n","saved = 0 # 저장된 이미지 수 카운트\n","\n","with torch.no_grad(): # 학습 아님 → gradient 계산 비활성화 (메모리/속도 절약)\n","    pbar = tqdm(total=N, desc=\"Generating fake images\") # 진행 상황 바 표시\n","    while saved < N:\n","        cur = min(bs, N - saved) # 남은 이미지 수가 bs보다 적으면 그만큼만 생성\n","        z = torch.randn(cur, nz, 1, 1, device=DEVICE) # 랜덤 노이즈 벡터 생성\n","        fake = netG(z).cpu() # Generator로 가짜 이미지 생성 (CPU로 이동)\n","\n","        # [-1,1] 범위를 [0,1] 범위로 변환 (이미지 저장용)\n","        fake = (fake*0.5 + 0.5).clamp(0,1)\n","        # 한 장씩 PNG로 저장\n","        for i in range(cur):\n","            save_image(fake[i], os.path.join(gen_dir, f\"fake_{saved+i:05d}.png\"))\n","        saved += cur # 저장된 개수 카운트 업데이트\n","        pbar.update(cur) # 진행바 업데이트\n","\n","print(\"생성 이미지 저장 완료:\", gen_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"id6e0nu377vL","executionInfo":{"status":"ok","timestamp":1755952746544,"user_tz":-540,"elapsed":3115,"user":{"displayName":"고현서","userId":"14122908031781224765"}},"outputId":"520f85b7-abce-4498-c611-518b44673ccc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Generating fake images: 100%|██████████| 2000/2000 [00:03<00:00, 697.04it/s]"]},{"output_type":"stream","name":"stdout","text":["생성 이미지 저장 완료: /content/gen_images\n"]}]},{"cell_type":"markdown","source":["### 분류 데이터셋/로더\n","- Images → label 1 (real)\n","- gen_images → label 0 (fake)"],"metadata":{"id":"HS5KHkn98IKJ"}},{"cell_type":"code","source":["# Real/Fake 이진 분류용 Dataset\n","class RealFakeDataset(Dataset):\n","    def __init__(self, real_root, fake_root, transform):\n","        # real_root 아래 모든 하위폴더까지 이미지 경로 수집\n","        self.real = sorted(glob.glob(os.path.join(real_root, \"*\"))) + \\\n","                    sorted(glob.glob(os.path.join(real_root, \"**\", \"*\"), recursive=True))\n","        # 이미지 확장자만 필터링\n","        self.real = [p for p in self.real if p.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\"))]\n","        # fake_root(생성 이미지 폴더)에서 PNG만 수집 (위에서 PNG로 저장했기 때문)\n","        self.fake = sorted(glob.glob(os.path.join(fake_root, \"*.png\")))\n","        self.transform = transform\n","        # (경로, 라벨) 튜플 리스트 생성: real=1, fake=0  ← 이진 분류 라벨\n","        self.items = [(p,1) for p in self.real] + [(p,0) for p in self.fake]\n","\n","    def __len__(self): return len(self.items)\n","    def __getitem__(self, idx):\n","        # idx번째 샘플 로드\n","        p, y = self.items[idx]\n","        img = Image.open(p).convert(\"RGB\") # RGB 통일(회색/알파 채널 제거)\n","        img = self.transform(img) # 분류기 입력 전처리 적용\n","        return img, y # 이미지 텐서, 라벨(int: 0 or 1) 반환\n","\n","# 분류기용 전처리(224 크기, ImageNet 표준화)\n","cls_tfm = transforms.Compose([\n","    transforms.Resize(256), # 짧은 변 256으로 리사이즈\n","    transforms.CenterCrop(224), # 중앙 224x224로 크롭 (ResNet 등 호환)\n","    transforms.ToTensor(), # [0,1] 텐서로 변환\n","    transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225)), # ImageNet 평균/표준편차로 정규화\n","])\n","\n","# 전체 Dataset 생성 및 Train/Val 분할\n","full_ds = RealFakeDataset(\"/content/Images\", \"/content/gen_images\", cls_tfm)\n","# 인덱스 셔플 후 8:2로 간단 분할 (층화X, 클래스 불균형 주의)\n","indices = list(range(len(full_ds)))\n","random.shuffle(indices)\n","split = int(0.8*len(indices))\n","tr_idx, va_idx = indices[:split], indices[split:]\n","# SubsetRandomSampler로 지정 인덱스만 뽑아 배치 구성\n","sampler_tr = torch.utils.data.SubsetRandomSampler(tr_idx)\n","sampler_va = torch.utils.data.SubsetRandomSampler(va_idx)\n","# DataLoader: 배치/워커/고정메모리 설정\n","dl_tr = DataLoader(full_ds, batch_size=64, sampler=sampler_tr, num_workers=2, pin_memory=True)\n","dl_va = DataLoader(full_ds, batch_size=64, sampler=sampler_va, num_workers=2, pin_memory=True)"],"metadata":{"id":"lD5kifI68TZk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 분류기(ResNet18) 학습"],"metadata":{"id":"vBovnaMr8YTo"}},{"cell_type":"code","source":["import torchvision.models as models\n","import torch.nn.functional as F\n","\n","# 1. 모델 정의 (ResNet18 전이학습)\n","cls_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","# 기존 ResNet18은 ImageNet 1000클래스 → 출력층(fc)을 2클래스(Real/Fake)로 교체\n","cls_model.fc = nn.Linear(cls_model.fc.in_features, 2)  # 2클래스(실제/가짜)\n","cls_model = cls_model.to(DEVICE)\n","\n","# 2. 옵티마이저 & 손실 함수\n","optimizer = optim.Adam(cls_model.parameters(), lr=1e-4) # Adam 최적화기\n","criterion_ce = nn.CrossEntropyLoss() # 다중 클래스 분류용 손실 (여기선 2클래스)\n","\n","# 3. 학습 루프 정의\n","def train_one_epoch():\n","    cls_model.train()          # 학습 모드 (Dropout/BatchNorm 활성화)\n","    tot, corr, n = 0.0, 0, 0   # 누적 loss, 정답 개수, 샘플 수\n","    for x, y in dl_tr:         # 학습 데이터 배치 단위 반복\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","        optimizer.zero_grad(set_to_none=True)     # gradient 초기화\n","        logits = cls_model(x)                     # 모델 forward → 예측 로짓 (B,2)\n","        loss = criterion_ce(logits, y)            # CrossEntropyLoss 계산\n","        loss.backward()                           # 역전파 (gradient 계산)\n","        optimizer.step()                          # weight 업데이트\n","        tot += loss.item()*y.size(0)              # 배치 손실 합산\n","        pred = logits.argmax(1)                   # 가장 큰 값이 예측 클래스\n","        corr += (pred==y).sum().item()            # 정답 개수 누적\n","        n += y.size(0)                            # 샘플 수 누적\n","    return tot/n, corr/n                          # 평균 손실, 정확도 반환\n","\n","@torch.no_grad()\n","def valid_one_epoch():\n","    cls_model.eval()         # 평가 모드 (Dropout/BatchNorm 고정)\n","    tot, corr, n = 0.0, 0, 0\n","    for x, y in dl_va:       # 검증 데이터 배치 단위 반복\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","        logits = cls_model(x)                     # forward\n","        loss = criterion_ce(logits, y)            # loss 계산\n","        tot += loss.item()*y.size(0)              # 손실 합산\n","        pred = logits.argmax(1)                   # 예측\n","        corr += (pred==y).sum().item()            # 정답 개수 합산\n","        n += y.size(0)\n","    return tot/n, corr/n                          # 평균 손실, 정확도 반환\n","\n","# 4. 학습 실행\n","EPOCHS = 5\n","for ep in range(1, EPOCHS+1):\n","    tl, ta = train_one_epoch() # train loss / accuracy\n","    vl, va = valid_one_epoch()  # valid loss / accuracy\n","    print(f\"[CLS {ep:02d}] train {tl:.3f}/{ta:.3f} | valid {vl:.3f}/{va:.3f}\")\n","\n","# 5. 학습된 모델 저장\n","torch.save(cls_model.state_dict(), os.path.join(OUT_DIR, \"real_vs_fake_resnet18.pt\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_kSsT4HW8bp8","executionInfo":{"status":"ok","timestamp":1755953600865,"user_tz":-540,"elapsed":361420,"user":{"displayName":"고현서","userId":"14122908031781224765"}},"outputId":"3177c625-8166-4600-c26e-c7555bc38934"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS 01] train 0.010/0.999 | valid 0.000/1.000\n","[CLS 02] train 0.000/1.000 | valid 0.000/1.000\n","[CLS 03] train 0.000/1.000 | valid 0.000/1.000\n","[CLS 04] train 0.000/1.000 | valid 0.000/1.000\n","[CLS 05] train 0.000/1.000 | valid 0.000/1.000\n"]}]},{"cell_type":"code","source":["from PIL import Image\n","import torch\n","import torch.nn.functional as F\n","\n","# 테스트할 이미지 경로 지정 (실제 or 생성)\n","# test_path = \"/content/Images/abraham-manievich_0.jpg\"  # 실제\n","test_path = \"/content/gen_images/fake_00000.png\"  # 생성\n","\n","# 1. 이미지 로드 & 전처리\n","img = Image.open(test_path).convert(\"RGB\") # 이미지 열기 + RGB 통일\n","img = cls_tfm(img).unsqueeze(0).to(DEVICE)  # 분류기용 전처리(224, 정규화) + 배치 차원 추가 + GPU로 이동\n","#   - unsqueeze(0): (C,H,W) → (1,C,H,W)로 바꿔서 '배치=1' 형태로 맞춤\n","\n","# 2. 분류기 추론 (inference)\n","cls_model.eval()            # 평가 모드 (Dropout/BatchNorm 고정)\n","with torch.no_grad():       # 추론이므로 gradient 계산 비활성화 (메모리/속도 절약)\n","    logits = cls_model(img) # 모델 forward → 출력 로짓 (1,2) → [가짜 점수, 실제 점수] # 1 → 배치 크기(batch size) = 1 (이미지 한 장만 넣었으니까), 2 → 클래스 개수(num classes) = 2 (Real vs Fake 분류니까)\n","    probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n","    # Softmax로 확률 변환 → numpy 배열로 변환 → [가짜확률, 실제확률]\n","\n","# 3. 결과 출력\n","print(\"실제일 확률:\", probs[1])  # 클래스 인덱스 1 = Real\n","print(\"가짜일 확률:\", probs[0])  # 클래스 인덱스 0 = Fake"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BJGQKdlz8g91","executionInfo":{"status":"ok","timestamp":1755953782502,"user_tz":-540,"elapsed":47,"user":{"displayName":"고현서","userId":"14122908031781224765"}},"outputId":"af36f254-3487-4d74-b430-548c861b865e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["실제일 확률: 9.180316e-12\n","가짜일 확률: 1.0\n"]}]}]}