{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BehwIW_Qnqe","executionInfo":{"status":"ok","timestamp":1756374642433,"user_tz":-540,"elapsed":2223,"user":{"displayName":"‍손단하[재학 / 경영학전공]","userId":"03748145936175732953"}},"outputId":"481d9155-60c1-43e4-dc4c-493e3c6f5e47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# !cp \"/content/drive/MyDrive/dcgan/dataset/archive.zip\" /content/"],"metadata":{"id":"wWLpPyMDJE2R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # /content/portrait_paintings 에 압축 해제\n","# !unzip -q /content/archive.zip -d /content/portrait_paintings"],"metadata":{"id":"HU4TmLu_JE-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # 압축 푼 이미지들을 portraits/ 하위로 이동 (혹은 복사)\n","# !cp -r /content/portrait_paintings/* \"/content/drive/MyDrive/dcgan/dataset/\""],"metadata":{"id":"dE0Y1qYZKJTJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. 코랩 환경 + 데이터 전처리/정규화 + 로더 확인"],"metadata":{"id":"KvKQi8Aamovg"}},{"cell_type":"code","source":["!pip -q install kornia"],"metadata":{"id":"AreYKdcGFsrB","executionInfo":{"status":"ok","timestamp":1756374646818,"user_tz":-540,"elapsed":4391,"user":{"displayName":"‍손단하[재학 / 경영학전공]","userId":"03748145936175732953"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"511d0624-94d7-4025-d6cd-4a9088cc2d9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m1.0/1.1 MB\u001b[0m \u001b[31m131.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import math, os, glob, re, json, time, random\n","import numpy as np\n","import torch, torchvision         # 파이토치와 이미지 처리 라이브러리\n","import torch.nn as nn             # 신경망 계층 정의\n","import matplotlib.pyplot as plt\n","import kornia.augmentation as K   # 이미지 증강 라이브러리\n","from pathlib import Path\n","from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader # 배치 단위 데이터 로더"],"metadata":{"id":"VDdfgTi-CGdZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"PyTorch :\", torch.__version__)\n","print(\"CUDA    :\", torch.cuda.is_available())\n","print(\"Torchvision :\", torchvision.__version__)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device  :\", device) # 실제 사용할 디바이스 정보 출력"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"stbTVCowCBjO","executionInfo":{"status":"ok","timestamp":1756374662485,"user_tz":-540,"elapsed":9,"user":{"displayName":"‍손단하[재학 / 경영학전공]","userId":"03748145936175732953"}},"outputId":"81d20eee-0266-4492-a16c-c71112e30254"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch : 2.8.0+cu126\n","CUDA    : True\n","Torchvision : 0.23.0+cu126\n","Device  : cuda\n"]}]},{"cell_type":"markdown","source":["## 폴더 생성\n","\n","- 체크포인트/샘플 이미지/로그 등을 Drive에 저장해서, 세션이 끊겨도 이어서 학습할 수 있게\n"],"metadata":{"id":"DNysCddSDPkR"}},{"cell_type":"code","source":["PROJECT_NAME = \"dcgan_portraits128\"  # 출력(결과) 폴더 이름\n","\n","# ---- 출력(체크포인트/샘플/로그) 루트 ----\n","DRIVE_ROOT_OUT = \"/content/drive/MyDrive/dcgan/\" # 결과물을 모아둘 상위 폴더\n","SAVE_DIR       = f\"{DRIVE_ROOT_OUT}/{PROJECT_NAME}\"       # 프로젝트 전용 출력 폴더\n","\n","# 하위 출력 폴더 (관리 편의)\n","CKPT_DIR   = f\"{SAVE_DIR}/ckpts\"                          # 체크포인트(.pt) 저장\n","SAMPLE_DIR = f\"{SAVE_DIR}/samples\"                        # 생성 샘플 이미지 저장\n","LOG_DIR    = f\"{SAVE_DIR}/logs\"                           # 로그/메모 등\n","\n","# ---- 데이터셋(ImageFolder) 경로 ----\n","DATA_DIR  = \"/content/drive/MyDrive/dcgan/datasets\"  # ImageFolder의 root\n","CLASS_DIR = f\"{DATA_DIR}/Images\"                          # 실제 이미지가 들어있는 '클래스' 폴더\n","\n","# 폴더 생성 (출력 전용)\n","os.makedirs(CKPT_DIR, exist_ok=True)                      # ckpts 폴더 생성\n","os.makedirs(SAMPLE_DIR, exist_ok=True)                    # samples 폴더 생성\n","os.makedirs(LOG_DIR, exist_ok=True)                       # logs 폴더 생성\n","\n","# 구조 검증 (안전장치)\n","DATA_DIR_PATH  = Path(DATA_DIR)\n","CLASS_DIR_PATH = Path(CLASS_DIR)\n","\n","assert DATA_DIR_PATH.exists(), f\"[경고] DATA_DIR가 존재하지 않습니다: {DATA_DIR}\"\n","assert CLASS_DIR_PATH.exists(), f\"[경고] CLASS_DIR가 존재하지 않습니다: {CLASS_DIR}\\n\" \\\n","                                f\"이미지가 '/DCGAN/datasets/images'에 있는지 확인하세요.\"\n","\n","has_any_image = any(CLASS_DIR_PATH.glob(\"*.jpg\")) or any(CLASS_DIR_PATH.glob(\"*.jpeg\")) or any(CLASS_DIR_PATH.glob(\"*.png\"))\n","assert has_any_image, f\"[경고] {CLASS_DIR} 내에 이미지(.jpg/.jpeg/.png)가 보이지 않습니다.\"\n","\n","# 경로 출력 (최종 확인)\n","print(\"SAVE_DIR   :\", SAVE_DIR)\n","print(\"CKPT_DIR   :\", CKPT_DIR)\n","print(\"SAMPLE_DIR :\", SAMPLE_DIR)\n","print(\"LOG_DIR    :\", LOG_DIR)\n","print(\"DATA_DIR   :\", DATA_DIR, \"(ImageFolder root)\")\n","print(\"CLASS_DIR  :\", CLASS_DIR,  \"(클래스 폴더)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K2uEcBn4CI7R","executionInfo":{"status":"ok","timestamp":1756374718004,"user_tz":-540,"elapsed":22370,"user":{"displayName":"‍손단하[재학 / 경영학전공]","userId":"03748145936175732953"}},"outputId":"e66f0440-3a08-4f43-fd28-18d9682ac584"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SAVE_DIR   : /content/drive/MyDrive/dcgan//dcgan_portraits128\n","CKPT_DIR   : /content/drive/MyDrive/dcgan//dcgan_portraits128/ckpts\n","SAMPLE_DIR : /content/drive/MyDrive/dcgan//dcgan_portraits128/samples\n","LOG_DIR    : /content/drive/MyDrive/dcgan//dcgan_portraits128/logs\n","DATA_DIR   : /content/drive/MyDrive/dcgan/datasets (ImageFolder root)\n","CLASS_DIR  : /content/drive/MyDrive/dcgan/datasets/Images (클래스 폴더)\n"]}]},{"cell_type":"markdown","source":["## 설정 값(에폭=100 포함) + 시드 고정\n","- 에폭: 100 → 1차 학습(100ep) 후에 품질 점검/튜닝 시작\n","- 이후 단계에서 체크포인트 매니저/훈련 루프를 연결할 것"],"metadata":{"id":"1mK7u4ieCI9x"}},{"cell_type":"code","source":["def seed_all(seed=42):\n","    # 시드(seed)는 무작위(random) 결과를 일정하게 고정하는 값임\n","    # → 모델 학습 중간에 중단 후 '체크포인트'에서 이어갈 때, 같은 결과를 재현하려면 필요함\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    # cuDNN 최적화 기능: 속도는 빠르지만 결과가 조금씩 달라질 수 있음\n","    # → 완전 재현성이 꼭 필요하다면 False로 두는 게 안전\n","    torch.backends.cudnn.benchmark = True\n","\n","# 시드값을 42로 고정 → 학습 시 항상 같은 조건에서 시작하게 됨\n","seed_all(42)"],"metadata":{"id":"DWR27E9-CJAQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 학습 설정값 모음 (Config) -> 여기를 변경"],"metadata":{"id":"9MEmfSAuaJdH"}},{"cell_type":"code","source":["CFG = dict(\n","    img_size=128,\n","    z_dim=128,         # 잠재공간(latent space) 차원. 노이즈 벡터 크기\n","    ngf=64,\n","    ndf=64,\n","    batch_size=64,\n","\n","    lr=2e-4, # 초반\n","\n","    lr_g=2e-4,  # 직전: 4e-4 -> 2.5e-4 (950 -> 965)\n","    lr_d=2.6e-5,   # 직전: 5e-5/ 2e-5→2.6e-5\n","\n","    # 학습률과 Adam 옵티마이저 파라미터 (DCGAN 논문 권장값)\n","    beta1=0.5, beta2=0.999,\n","\n","    epochs=1500,\n","    device=device\n",")\n","CFG"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hw6hgFTgaIIn","executionInfo":{"status":"ok","timestamp":1756377662153,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍손단하[재학 / 경영학전공]","userId":"03748145936175732953"}},"outputId":"abf82fee-8c2e-4c9d-b241-30e93b6fc2fc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'img_size': 128,\n"," 'z_dim': 128,\n"," 'ngf': 64,\n"," 'ndf': 64,\n"," 'batch_size': 64,\n"," 'lr': 0.0002,\n"," 'lr_g': 0.0002,\n"," 'lr_d': 2.6e-05,\n"," 'beta1': 0.5,\n"," 'beta2': 0.999,\n"," 'epochs': 1500,\n"," 'device': 'cuda'}"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["학습률 변화 기록\n","\n","- Generator 학습률: 2e-4 -> 2.5e-4 -> 3e-4 -> 4e-4\n","- Discriminator 학습률 : 1.5e-4 -> 1e-4 -> 7e-5 -> 5e-5\n","\n","950 에포크에서 1000 에포크 학습하는 동안 사용한 학습률\n","- lr_g=4e-4\n","- lr_d=5e-5\n","\n","학습 상태\n","- 판별자의 로스값이 0.5에서 +-0.2 정도로 진동하는 반면 생성자는 2에서 6사의 값으로 진동함. 3, 4의 빈도가 높았음\n","\n","문제 상황\n","- 모드 붕괴(mode collapse)\n","\n","- 해석 : 판별자가 과적합/포화 직전 균형, 이때 G가 학습 신호를 충분히 못 받는것 같음\n","\n","- 현재 목표. 950의 이미지 확인. -> 후반부 디테일(눈·코·입) 수렴하는 것을 목적으로 함. (학습 쌓아 생성자의 세부 묘사를 이끌기)\n"],"metadata":{"id":"GZlAfM759L9f"}},{"cell_type":"code","source":["CFG.update({\n","    \"use_inst_noise\": True,   # 인스턴스 노이즈 켜기/끄기\n","    \"inst_noise_sigma\": 0.10, # 시작 σ (예: 0.05→0.10로 강화)\n","})"],"metadata":{"id":"yQBO2y0ObI7q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InstanceNoise(nn.Module):\n","    \"\"\"\n","    D 입력으로 들어가기 직전에 x += N(0, sigma) 형태로 가우시안 노이즈를 더해주는 모듈.\n","    real/fake 둘 다에 똑같이 적용해야 균형이 깨지지 않습니다.\n","    \"\"\"\n","    def __init__(self, sigma=0.05):\n","        super().__init__()\n","        self.sigma = sigma\n","\n","    def forward(self, x):\n","        if self.sigma <= 0:\n","            return x\n","        return x + torch.randn_like(x) * self.sigma\n"],"metadata":{"id":"ZR_Bz5M2bSW-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 데이터로더\n","\n","- 1) DataLoader에서 사용하는 \"비미분\" 전처리/증강\n","    - 여기서는 학습 전에 이미지를 고정된 방식으로 가공함\n","    - GAN은 마지막에 Tanh 활성화를 쓰므로 [-1,1] 범위로 정규화하는 게 핵심"],"metadata":{"id":"xtWk4O-CCJC4"}},{"cell_type":"code","source":["def make_loader_transforms(img_size=128):\n","    resize_to = int(img_size * 1.125)  # 입력 크기보다 약간 크게 리사이즈 후 센터크롭 (128px → 약 144~160px)\n","    return transforms.Compose([\n","        transforms.Resize(resize_to),                        # 이미지 크기를 키움\n","        transforms.CenterCrop(img_size),                     # 중앙 기준으로 원하는 크기만큼 자름\n","        transforms.RandomHorizontalFlip(p=0.5),              # 50% 확률로 좌우 반전\n","        transforms.ColorJitter(brightness=0.10, contrast=0.10,\n","                               saturation=0.05, hue=0.02),   # 색상/밝기/대비를 약하게 랜덤 조정\n","        transforms.ToTensor(),                               # [H,W,C] 이미지를 [C,H,W] 텐서로 변환 + [0,1]로 정규화\n","        transforms.Normalize((0.5,)*3, (0.5,)*3),            # [0,1] → [-1,1] 범위로 변환\n","    ])"],"metadata":{"id":"6B0c1ZmyXR_5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 미분가능 증강\n","- 판별기 입력 직전에 적용\n","- D가 너무 강한 상황이니 DiffAug를 “조금 더” 강하게 해서 D를 덜 확신하게 만들면 G가 숨 쉴 공간이 생김"],"metadata":{"id":"LQ5QOQEiXUTK"}},{"cell_type":"code","source":["class DiffAug(nn.Module):\n","    \"\"\"\n","    미분가능 증강.\n","    - 'recovery': 디테일 회복용(색/블러/노이즈 OFF, 아주 약한 기하)\n","    - 'mild'/'medium'/'strong': 점진적 강화\n","    \"\"\"\n","    def __init__(self, img_size=128, strength=\"medium\"):\n","        super().__init__()\n","        self.img_size = img_size\n","        self.set_strength(strength)\n","\n","    def set_strength(self, strength=\"medium\"):\n","        s = str(strength).lower()\n","        if s not in {\"recovery\", \"mild\", \"medium\", \"strong\"}:\n","            s = \"medium\"\n","\n","        # 1) 기본값(모든 분기에서 공통으로 존재하도록 미리 선언)\n","        p_flip      = 0.5\n","        aff_deg     = 2.0\n","        aff_tr      = 0.02\n","        aff_scale   = (0.99, 1.01)\n","        aff_p       = 0.5\n","\n","        use_color   = False; b_delta = 0.0; c_delta = 0.0; bc_p = 0.0\n","        use_blur    = False; blur_p  = 0.0; blur_ks = (3, 3)\n","        use_noise   = False; noise_p = 0.0; noise_std = 0.01\n","\n","        # 2) 강도별로 값만 수정\n","        if s == \"recovery\":\n","            p_flip = 0.5\n","            aff_deg, aff_tr, aff_scale, aff_p = 2.0, 0.02, (0.99, 1.01), 0.5\n","            # 나머지(use_color/use_blur/use_noise)는 False 유지\n","\n","        elif s == \"mild\":\n","            p_flip = 0.6\n","            aff_deg, aff_tr, aff_scale, aff_p = 3.0, 0.03, (0.98, 1.03), 0.6\n","            use_color, b_delta, c_delta, bc_p = True, 0.06, 0.06, 0.3\n","\n","        elif s == \"strong\":\n","            p_flip = 0.8\n","            aff_deg, aff_tr, aff_scale, aff_p = 6.0, 0.05, (0.95, 1.05), 0.8\n","            use_color, b_delta, c_delta, bc_p = True, 0.10, 0.10, 0.5\n","            use_blur, blur_p, blur_ks        = True, 0.30, (3, 3)\n","            use_noise, noise_p, noise_std    = True, 0.20, 0.01\n","\n","        else:  # medium\n","            p_flip = 0.7\n","            aff_deg, aff_tr, aff_scale, aff_p = 4.0, 0.04, (0.97, 1.04), 0.7\n","            use_color, b_delta, c_delta, bc_p = True, 0.08, 0.08, 0.4\n","            use_blur, blur_p, blur_ks        = True, 0.15, (3, 3)\n","            use_noise, noise_p, noise_std    = True, 0.10, 0.01\n","\n","        # 3) ops 구성(색/블러/노이즈는 flag에 따라 조건부 추가)\n","        ops = [\n","            K.RandomHorizontalFlip(p=p_flip),\n","            K.RandomAffine(\n","                degrees=aff_deg,\n","                translate=(aff_tr, aff_tr),\n","                scale=aff_scale,\n","                p=aff_p,\n","                align_corners=False,\n","            ),\n","        ]\n","        if use_color:\n","            ops += [\n","                K.RandomBrightness(b_delta, p=bc_p),\n","                K.RandomContrast(c_delta, p=bc_p),\n","            ]\n","        if use_blur and blur_p > 0:\n","            ops.append(K.RandomGaussianBlur(kernel_size=blur_ks, sigma=(0.1, 1.0), p=blur_p))\n","        if use_noise and noise_p > 0:\n","            ops.append(K.RandomGaussianNoise(mean=0.0, std=noise_std, p=noise_p))\n","\n","        self.augs = nn.Sequential(*ops)\n","        self._strength = s\n","\n","    def forward(self, x):\n","        return self.augs(x)\n"],"metadata":{"id":"byEOOrDJgKtr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def adjust_diffaug(diffaug, d_val, g_val, lf, global_step,\n","                   last_change_step, cool_down=600):\n","    \"\"\"\n","    d_val: D total loss (BCE+R1 포함이면 가능하면 'pure D loss'로 넣는 게 더 깔끔)\n","    g_val: G loss\n","    lf   : D의 fake loss (가짜를 얼마나 쉽게 잡는지)\n","    \"\"\"\n","    # 전환 너무 잦지 않게\n","    if global_step - last_change_step < cool_down:\n","        return last_change_step  # 유지\n","\n","    curr = diffaug.strength\n","    target = curr\n","\n","    # --- 우선순위 1: 체커보드/고주파 과잉 의심 구간 ---\n","    # 가짜를 너무 쉽게 잡음 + G가 흔들리는 패턴일 때는 증강 최소화\n","    if lf < 0.05 and g_val > 1.6:\n","        target = \"recovery\"\n","\n","    # --- 균형 범위: mild 유지 ---\n","    elif 1.0 <= d_val <= 1.6 and 0.8 <= g_val <= 2.2:\n","        target = \"mild\"\n","\n","    # --- D가 너무 약하고(G가 이김) 오버피팅/저주파 수렴 위험: medium까지 ---\n","    elif d_val < 0.9 and g_val > 2.5:\n","        target = \"medium\"\n","\n","    # 그 외엔 안전하게 recovery로 수렴\n","    else:\n","        target = \"recovery\"\n","\n","    # strong은 금지(체커보드 시기)\n","    if target == \"strong\":\n","        target = \"medium\"\n","\n","    if target != curr:\n","        diffaug.set_strength(target)\n","        return global_step  # 변경된 시점 기록\n","    return last_change_step\n"],"metadata":{"id":"h-xWBoRd7jPf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 숨기기\n"],"metadata":{"id":"RU5qTSH5gIkE"}},{"cell_type":"code","source":["# class DiffAug(nn.Module):\n","#     \"\"\"\n","#     미분가능 증강. 'strength'로 강도를 제어하고, 필요 시 에폭에 따라 동적으로 조절할 수 있게 설계.\n","#     회화 데이터 특성상 색/기하 변환은 '과하지 않게'가 원칙.\n","#     \"\"\"\n","#     def __init__(self, img_size=128, strength=\"medium\"):\n","#         super().__init__()\n","#         self.img_size = img_size\n","#         self.set_strength(strength)  # 내부적으로 self.augs를 구성\n","\n","#     def set_strength(self, strength=\"medium\"):\n","#         strength = strength.lower()\n","#         if strength not in {\"mild\", \"medium\", \"strong\", \"recovery\"}:\n","#             strength = \"medium\"\n","\n","#         if strength == \"recovery\":\n","#             # 선명도 회복용: 색/블러/노이즈 OFF, 아주 약한 기하 변환만\n","#             p_flip = 0.5\n","#             aff_deg, aff_tr, aff_scale, aff_p = 2.0, 0.02, (0.99, 1.01), 0.5\n","#             use_color = False\n","#             use_blur  = False\n","#             use_noise = False\n","\n","#         if strength == \"mild\":\n","#             p_flip = 0.6\n","#             aff_deg, aff_tr, aff_scale, aff_p = 3.0, 0.03, (0.98, 1.03), 0.6\n","#             b_delta, c_delta, bc_p = 0.06, 0.06, 0.3\n","#             blur_p, blur_ks = 0.0, (3, 3)       # 기본은 블러 없음\n","#             noise_p, noise_std = 0.0, 0.01      # 기본은 노이즈 없음\n","\n","#         elif strength == \"strong\":\n","#             p_flip = 0.8\n","#             aff_deg, aff_tr, aff_scale, aff_p = 6.0, 0.05, (0.95, 1.05), 0.8\n","#             b_delta, c_delta, bc_p = 0.10, 0.10, 0.5\n","#             blur_p, blur_ks = 0.3, (3, 3)       # 약한 가우시안 블러 추가 (회화 질감 보존 위해 약하게)\n","#             noise_p, noise_std = 0.2, 0.01      # 약한 가우시안 노이즈 (너무 세게 X)\n","\n","#         else:  # medium\n","#             p_flip = 0.7\n","#             aff_deg, aff_tr, aff_scale, aff_p = 4.0, 0.04, (0.97, 1.04), 0.7\n","#             b_delta, c_delta, bc_p = 0.08, 0.08, 0.4\n","#             blur_p, blur_ks = 0.15, (3, 3)\n","#             noise_p, noise_std = 0.1, 0.01\n","\n","#         # Kornia augs (모두 미분가능)\n","#         ops = [\n","#             K.RandomHorizontalFlip(p=p_flip),\n","#             K.RandomAffine(degrees=aff_deg, translate=(aff_tr, aff_tr),\n","#                            scale=aff_scale, p=aff_p),\n","#             K.RandomBrightness(b_delta, p=bc_p),\n","#             K.RandomContrast(c_delta, p=bc_p),\n","#         ]\n","#         if strength != \"recovery\" and use_color:\n","#             # 색 변환은 회화에 과하면 디테일 손실 → 회복 단계에서는 OFF\n","#             ops += [K.RandomBrightness(b_delta, p=bc_p),\n","#                     K.RandomContrast(c_delta, p=bc_p)]\n","#         if strength != \"recovery\" and use_blur and blur_p > 0:\n","#             ops.append(K.RandomGaussianBlur(kernel_size=blur_ks, sigma=(0.1, 1.0), p=blur_p))\n","#         if strength != \"recovery\" and use_noise and noise_p > 0:\n","#             ops.append(K.RandomGaussianNoise(mean=0.0, std=noise_std, p=noise_p))\n","#         # if blur_p > 0:\n","#         #     ops.append(K.RandomGaussianBlur(kernel_size=blur_ks, sigma=(0.1, 1.0), p=blur_p))\n","#         # if noise_p > 0:\n","#         #     ops.append(K.RandomGaussianNoise(mean=0.0, std=noise_std, p=noise_p))\n","\n","#         self.augs = nn.Sequential(*ops)\n","#         self._strength = strength\n","\n","#     @torch.no_grad()\n","#     def example_show(self, x):\n","#         return self.augs(x)\n","\n","#     def forward(self, x):\n","#         return self.augs(x)"],"metadata":{"id":"_VehqaiJCJFI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 체크포인트 저장 주기 정책"],"metadata":{"id":"-57r7izLYkiR"}},{"cell_type":"code","source":["def should_save_ckpt(epoch: int, max_epoch_plan: int = 500) -> bool:\n","    \"\"\"\n","    epoch: 현재 1부터 증가한다고 가정.\n","    max_epoch_plan: 전체 계획된 마지막 에폭(예: 500)\n","    \"\"\"\n","    if epoch <= 100:\n","        return (epoch % 20 == 0)  # 20, 40, 60, 80, 100\n","    else:\n","        # 101~max_epoch_plan: 5 에폭마다\n","        return (epoch % 5 == 0) and (epoch <= max_epoch_plan)\n","\n","# 간단 테스트\n","# for e in [1,20,40,99,100,101,105,110,500,501]:\n","#     if should_save_ckpt(e, max_epoch_plan=500):\n","#         print(\"save @\", e)\n"],"metadata":{"id":"cg19cdHWCJHg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [Step 4-C] Matplotlib로 에폭마다 샘플 저장 헬퍼"],"metadata":{"id":"HZ5VoQW-dHhI"}},{"cell_type":"code","source":["def denorm_to_numpy(t):\n","    # 학습에 쓰던 텐서를 시각화/저장하기 위해 numpy 배열로 변환하는 함수\n","    t = t.detach().cpu().clamp_(-1, 1)    # 학습 그래프(detach) 끊고 CPU로 옮긴 뒤 값 범위를 [-1,1]로 자름\n","    t = (t + 1.0) / 2.0                   # [-1,1] → [0,1] 범위로 변환 (이미지 표현용)\n","    t = t.permute(0, 2, 3, 1).numpy()     # (B,C,H,W) → (B,H,W,C) 로 차원 순서 변경 후 numpy 배열로 변환\n","    return t\n","\n","def save_grid_matplotlib(tensor_bchw, save_path, nrow=8, title=None):\n","    # 여러 이미지를 격자(grid) 형태로 저장하는 함수\n","    arr = denorm_to_numpy(tensor_bchw)    # 텐서를 numpy 이미지 배열로 변환\n","    B = arr.shape[0]                      # 이미지 개수(batch 크기)\n","    ncol = nrow                           # 열 개수 (기본 8)\n","    nrow_actual = math.ceil(B / ncol)     # 필요한 행 개수 계산\n","\n","    # matplotlib 서브플롯 생성 (행×열 구조)\n","    fig, axes = plt.subplots(nrow_actual, ncol, figsize=(ncol*2, nrow_actual*2))\n","    axes = np.atleast_2d(axes)            # axes를 2차원 배열로 맞춰줌 (행/열 구조 고정)\n","\n","    idx = 0\n","    for r in range(nrow_actual):          # 각 행 순회\n","        for c in range(ncol):             # 각 열 순회\n","            ax = axes[r, c]\n","            ax.axis('off')                # 축(좌표 눈금) 숨기기\n","            if idx < B:                   # 이미지가 남아있으면 출력\n","                ax.imshow(arr[idx])       # idx번째 이미지 표시\n","            idx += 1\n","\n","    if title is not None:                 # 전체 제목 옵션\n","        plt.suptitle(title)\n","    plt.tight_layout()                    # 레이아웃 자동 정리 (간격 조정)\n","    os.makedirs(os.path.dirname(save_path), exist_ok=True)  # 저장 경로 없으면 생성\n","    plt.savefig(save_path, dpi=150)       # 이미지 저장 (해상도 150dpi)\n","    plt.close(fig)                        # 메모리 절약 위해 figure 닫기\n"],"metadata":{"id":"VJfnh53OCJJo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [Step 4-D] 성능 vs 재현성 설정 예시"],"metadata":{"id":"XwS7p2HIh6hz"}},{"cell_type":"code","source":["def set_performance_mode():\n","    # 빠른 훈련 모드 (권장 기본값)\n","    # - 입력 이미지 크기가 고정이라면 cuDNN이 최적화된 알고리즘을 골라 성능 ↑\n","    torch.backends.cudnn.benchmark = True     # 여러 알고리즘 중 가장 빠른 것 자동 선택\n","    torch.backends.cudnn.deterministic = False # 결과가 실행마다 달라질 수 있음 (속도 우선)\n","    try:\n","        torch.use_deterministic_algorithms(False) # 가능한 경우 비결정적 알고리즘 허용\n","    except Exception:\n","        pass\n","    print(\"[Mode] Performance (benchmark=True)\")\n","\n","def set_strict_deterministic_mode():\n","    # 엄격한 재현성 모드\n","    # - 속도는 느리지만, 실행할 때마다 똑같은 결과가 나옴 (bit 단위 동일)\n","    torch.backends.cudnn.benchmark = False    # 빠른 알고리즘 탐색 끔\n","    torch.backends.cudnn.deterministic = True # 항상 같은 알고리즘만 사용\n","    try:\n","        torch.use_deterministic_algorithms(True) # 모든 연산에서 결정적 알고리즘 강제\n","    except Exception:\n","        pass\n","    print(\"[Mode] Strict Deterministic (benchmark=False, deterministic=True)\")\n","\n","# 기본적으로는 성능 모드 권장 (GAN 학습은 속도/안정성이 중요하기 때문)\n","set_performance_mode()\n","\n","# 만약 \"실험을 다시 돌려도 완전히 같은 결과\"가 꼭 필요하다면 아래로 교체:\n","# set_strict_deterministic_mode()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eLwMDFeeCJMA","executionInfo":{"status":"ok","timestamp":1756377670067,"user_tz":-540,"elapsed":9,"user":{"displayName":"‍손단하[재학 / 경영학전공]","userId":"03748145936175732953"}},"outputId":"5694283b-92fc-4592-b2c8-610b6e6c6a12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Mode] Performance (benchmark=True)\n"]}]},{"cell_type":"markdown","source":["## 모델 정의"],"metadata":{"id":"aAnh0O6EHEhA"}},{"cell_type":"code","source":["# 성능 모드: 입력크기 고정일 때 빠름 (완전 재현성 필요하면 4단계의 deterministic 모드로 변경) ----\n","set_performance_mode()  # cuDNN benchmark 활성화하여 속도 우선(입력 크기 고정일 때 효과적)\n","\n","# ---- 데이터셋/로더 (4단계의 make_loader_transforms 사용) ----\n","TFM = make_loader_transforms(CFG[\"img_size\"])  # 정규화([-1,1]) 및 약한 증강(Flip/ColorJitter/CenterCrop) 포함된 변환 생성\n","\n","# 여기서는 DATA_DIR 아래에 'Images' 폴더가 클래스 폴더로 인식되어 하나의 클래스가 됨.\n","ds = datasets.ImageFolder(root=DATA_DIR, transform=TFM)\n","\n","# 학습용 DataLoader 구성\n","loader = DataLoader(\n","    ds,                                # 위에서 만든 ImageFolder 데이터셋\n","    batch_size=CFG[\"batch_size\"],      # 배치 크기(메모리에 맞춰 조정)\n","    shuffle=True,                      # 매 epoch마다 샘플 섞기\n","    num_workers=4,                     # 병렬 로딩 워커 수(Colab에서는 2~4 권장)\n","    pin_memory=True,                   # CUDA 전송 최적화( GPU 사용 시 권장 )\n","    drop_last=True                     # 마지막 배치가 작으면 버려서 배치 크기 일정하게 유지\n",")\n","\n","# 로더/데이터셋 정보 출력(클래스 확인 포함)\n","print(f\"[Data] root       = {DATA_DIR}\")          # 현재 ImageFolder root 경로\n","print(f\"[Data] class_dir  = {CLASS_DIR}\")         # 실제 이미지가 들어있는 폴더(검증됨)\n","print(f\"[Data] class_names= {ds.classes}\")        # 예상: ['Images']\n","print(f\"[Data] images     = {len(ds)}\")           # 전체 이미지 수\n","print(f\"[Data] batches/ep = {len(loader)}\")       # 에폭당 배치 수"],"metadata":{"id":"1WwRZVXJIVch","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756377671211,"user_tz":-540,"elapsed":84,"user":{"displayName":"‍손단하[재학 / 경영학전공]","userId":"03748145936175732953"}},"outputId":"8b74040f-5e26-49f2-ad2f-14ec497d9d2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Mode] Performance (benchmark=True)\n","[Data] root       = /content/drive/MyDrive/dcgan/datasets\n","[Data] class_dir  = /content/drive/MyDrive/dcgan/datasets/Images\n","[Data] class_names= ['Images']\n","[Data] images     = 5734\n","[Data] batches/ep = 89\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# 판별자 정의\n","class Generator(nn.Module):\n","    def __init__(self, z_dim=128, ngf=64, nc=3):\n","        # z_dim : 잠재 벡터 차원\n","        # ngf   : feature map 크기(scale), generator 채널 폭 결정\n","        # nc    : 출력 채널 수 (컬러 이미지라면 3)\n","\n","        super().__init__()\n","        self.main = nn.Sequential(\n","            # 4x4\n","            # 입력: (z_dim) 크기의 잠재벡터 → ConvTranspose2d로 4x4 feature map 생성\n","            nn.ConvTranspose2d(z_dim, ngf*8, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf*8),  # 채널 정규화 → 학습 안정화\n","            nn.ReLU(True),          # 활성화 함수(ReLU)\n","\n","            # 4x4 → 8x8\n","            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf*4), nn.ReLU(True),\n","\n","            # 16x16\n","            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf*2), nn.ReLU(True),\n","\n","            # 32x32\n","            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf), nn.ReLU(True),\n","\n","            # 64x64\n","            nn.ConvTranspose2d(ngf, ngf//2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf//2), nn.ReLU(True),\n","\n","            # 128x128\n","            nn.ConvTranspose2d(ngf//2, nc, 4, 2, 1, bias=False),\n","            nn.Tanh(),  # 출력 범위 [-1,1]\n","        )\n","    def forward(self, z):\n","        # z : (batch, z_dim, 1, 1) 크기의 잠재 벡터 입력\n","        return self.main(z) # 출력: (batch, nc, 128, 128)"],"metadata":{"id":"K6RVn9Ys0DIZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 생성자 정의\n","class Discriminator(nn.Module):\n","    def __init__(self, ndf=64, nc=3):\n","        \"\"\"\n","        ndf : D의 채널 폭 스케일(64/96/128 등)\n","        nc  : 입력 채널 수(컬러=3)\n","        \"\"\"\n","        super().__init__()\n","\n","        def block(in_c, out_c, bn=True):\n","            \"\"\"\n","            공용 다운샘플 블록\n","            Conv(stride=2)로 해상도를 절반으로 줄이고,\n","            LeakyReLU로 비선형성, (선택) BatchNorm으로 학습 안정화\n","            \"\"\"\n","            layers = [nn.Conv2d(in_c, out_c, 4, 2, 1, bias=False),  # k=4, s=2, p=1 → H,W 절반\n","                      nn.LeakyReLU(0.2, inplace=True)]\n","            if bn:\n","                # DCGAN 권장: 첫 블록 제외하고는 BN 사용\n","                layers.insert(1, nn.BatchNorm2d(out_c))\n","            return nn.Sequential(*layers)\n","\n","        self.main = nn.Sequential(\n","            # 입력: (B, nc, 128, 128)\n","            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),  # 128x128 → 64x64  (주석 수정 포인트!)\n","            nn.LeakyReLU(0.2, inplace=True),          # 첫 블록은 BN 없음(논문 권장)\n","\n","            block(ndf,   ndf*2, bn=True),  # 64x64  → 32x32\n","            block(ndf*2, ndf*4, bn=True),  # 32x32  → 16x16\n","            block(ndf*4, ndf*8, bn=True),  # 16x16  →  8x8\n","\n","            # 8 -> 4  (한 층 추가해서 4x4 만들기)\n","            nn.Conv2d(ndf*8, ndf*16, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ndf*16),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            # 4 -> 1\n","            nn.Conv2d(ndf*16, 1, 4, 1, 0, bias=False)\n","        )\n","\n","    def forward(self, x):\n","        # 출력: (B,) 스칼라 로짓\n","        return self.main(x).view(-1)"],"metadata":{"id":"StkG2uqU0DQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 초기화 함수\n","def weights_init_dcgan(m):\n","    \"\"\"\n","    DCGAN 권장 초기화:\n","    - Conv/ConvTranspose: N(0, 0.02)\n","    - BatchNorm.weight  : N(1, 0.02)\n","      BatchNorm.bias    : 0\n","    목적: 초기에 각 층의 출력 분포가 안정되도록 하여\n","         G/D 모두에서 그래디언트 흐름이 막히지 않게 함.\n","    \"\"\"\n","    classname = m.__class__.__name__\n","\n","    if 'Conv' in classname or 'ConvTranspose' in classname:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","\n","    elif 'BatchNorm' in classname:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.zeros_(m.bias.data)"],"metadata":{"id":"goQkwWwn0KtC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 미분가능 증강\n","diffaug = DiffAug(img_size=CFG[\"img_size\"]).to(CFG[\"device\"])"],"metadata":{"id":"rZVDEcc70XGa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 체크포인트"],"metadata":{"id":"JX_1HSk_LIn-"}},{"cell_type":"code","source":["def latest_ckpt_path(save_dir: str):\n","    \"\"\"\n","    save_dir(=CKPT_DIR)에서 'ckpt_e*_s*.pt' 패턴을 정렬하여 가장 최신 경로 반환.\n","    latest.txt가 있으면 그 경로를 우선 사용.\n","    \"\"\"\n","    cand = sorted(glob.glob(os.path.join(save_dir, \"ckpt_e*_s*.pt\")))\n","    if not cand:\n","        lat = os.path.join(save_dir, \"latest.txt\")\n","        if os.path.exists(lat):\n","            p = Path(lat).read().strip()\n","            return p if os.path.exists(p) else None\n","        return None\n","    return cand[-1]\n","\n","def save_ckpt(epoch, step, netG, netD, optG, optD):\n","    \"\"\"\n","    원자적 저장(.tmp → rename)으로 중단/전원off 상황에서도 파일 무결성 보장.\n","    latest.txt에 방금 저장한 경로를 기록해 이어학습이 자동으로 최신을 로드.\n","    \"\"\"\n","    # ⬇️ CKPT_DIR로 변경\n","    fname = f\"ckpt_e{epoch:03d}_s{step:06d}.pt\"\n","    path  = os.path.join(CKPT_DIR, fname)\n","    tmp   = path + \".tmp\"\n","\n","    torch.save({\n","        \"epoch\": epoch, \"step\": step,\n","        \"G\": netG.state_dict(),\n","        \"D\": netD.state_dict(),\n","        \"optG\": optG.state_dict(),\n","        \"optD\": optD.state_dict(),\n","        \"cfg\": CFG,\n","    }, tmp)\n","    os.replace(tmp, path)\n","\n","    # latest 포인터 업데이트 (CKPT_DIR 안에 저장)\n","    with open(os.path.join(CKPT_DIR, \"latest.txt\"), \"w\") as f:\n","        f.write(path)\n","\n","    print(f\"[CKPT] saved: {path}\")\n","    return path\n","\n","def load_ckpt_if_any(netG, netD, optG, optD):\n","    \"\"\"\n","    CKPT_DIR에서 최신 체크포인트를 찾아 로드.\n","    없으면 (0,0) 반환 → 스크래치 학습 시작.\n","    \"\"\"\n","    path = latest_ckpt_path(CKPT_DIR)  # ⬅️ CKPT_DIR 사용\n","    if path is None:\n","        print(\"[CKPT] no checkpoint, start fresh.\")\n","        return 0, 0\n","\n","    ckpt = torch.load(path, map_location=CFG[\"device\"])\n","    netG.load_state_dict(ckpt[\"G\"])\n","    netD.load_state_dict(ckpt[\"D\"])\n","    optG.load_state_dict(ckpt[\"optG\"])\n","    optD.load_state_dict(ckpt[\"optD\"])\n","    start_epoch = int(ckpt.get(\"epoch\", 0))\n","    start_step  = int(ckpt.get(\"step\", 0))\n","\n","    print(f\"[CKPT] loaded: {path} (epoch {start_epoch}, step {start_step})\")\n","    return start_epoch, start_step\n"],"metadata":{"id":"TbQyJfxGHEmo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 학습 루프"],"metadata":{"id":"krGO2-aW5vsB"}},{"cell_type":"code","source":["def train_dcgan(max_epoch_plan=2000):\n","    device = CFG[\"device\"]\n","\n","    # 모델/옵티마/로스 구성\n","    netG = Generator(CFG[\"z_dim\"], CFG[\"ngf\"]).to(device)  # Generator 생성 후 디바이스로 이동\n","    netD = Discriminator(CFG[\"ndf\"]).to(device)            # Discriminator 생성 후 디바이스로 이동\n","    netG.apply(weights_init_dcgan); netD.apply(weights_init_dcgan)  # DCGAN 권장 가중치 초기화 적용\n","\n","    criterion = nn.BCEWithLogitsLoss()               # 시그모이드 내장형 BCE (안정적)\n","\n","    # TTUR: G/D 학습률 분리 (CFG에 lr_g, lr_d 존재)\n","    optG = torch.optim.Adam(                         # G 옵티마이저(Adam)\n","        netG.parameters(), lr=CFG[\"lr_g\"],\n","        betas=(CFG[\"beta1\"], CFG[\"beta2\"])\n","    )\n","    optD = torch.optim.Adam(                         # D 옵티마이저(Adam)\n","        netD.parameters(), lr=CFG[\"lr_d\"],\n","        betas=(CFG[\"beta1\"], CFG[\"beta2\"])\n","    )\n","\n","\n","    # DiffAug / Instance Noise\n","    diffaug = DiffAug(img_size=CFG[\"img_size\"],\n","                      strength=CFG.get(\"diffaug_strength\", \"medium\")).to(device)\n","    inst_noise = InstanceNoise(CFG[\"inst_noise_sigma\"]).to(device) if CFG.get(\"use_inst_noise\", False) else nn.Identity()\n","\n","\n","    # 이어학습: 최신 체크포인트가 있으면 로드\n","    start_epoch, global_step = load_ckpt_if_any(netG, netD, optG, optD)  # 없으면 (0,0) 반환 → 스크래치 시작\n","    z_fixed = torch.randn(64, CFG[\"z_dim\"], 1, 1, device=device)  # 8x8 그리드로 보기 좋게 64장\n","\n","    real_val = 0.9 # 라벨 스무딩: real=0.9 (fake는 0.0, gen 목표는 1.0)\n","\n","    print(f\"[Train] start from epoch={start_epoch+1}, steps={global_step}\")  # 재개 지점 로그\n","\n","\n","    # 에폭 루프 (예: 처음엔 100, 이후 500까지 재실행)\n","    for epoch in range(start_epoch + 1, CFG[\"epochs\"] + 1):\n","        netG.train(); netD.train() # 학습 모드로 전환(BN/Dropout 등)\n","\n","        # Instance Noise: σ 선형 감쇠(느리게)  ⇒ epochs * 1.5\n","        if isinstance(inst_noise, InstanceNoise):\n","            inst_noise.sigma = max(\n","                0.02, CFG[\"inst_noise_sigma\"] * (1 - epoch / (CFG[\"epochs\"] * 1.5))\n","            )\n","\n","        # DiffAug 강도 스테이지 스케줄 (초중반 강하게, 후반 완화)\n","        # T = CFG[\"epochs\"]\n","        # p = epoch / T\n","        # if p < 0.45:\n","        #     diffaug.set_strength(\"strong\")   # 초반: D를 어렵게 → G가 숨 쉴 공간 확보\n","        # elif p < 0.80:\n","        #     diffaug.set_strength(\"medium\")   # 중반: 균형 유지\n","        # else:\n","        #     diffaug.set_strength(\"mild\")     # 후반: 디테일 수렴 방해 최소화\n","\n","\n","        # 미니배치 루프\n","        for real, _ in loader:\n","            real = real.to(device)\n","            B = real.size(0)\n","\n","            # 스킵분기에서 로깅 변수 초기화\n","            loss_real = torch.tensor(0.0, device=device)\n","            loss_fake = torch.tensor(0.0, device=device)\n","            loss_D    = torch.tensor(0.0, device=device)\n","\n","            # D 업데이트 빈도 설정\n","            do_update_D = (global_step % 4 == 0) # 4스텝 중 1번만\n","\n","            # --------- Discriminator 업데이트 ---------\n","            if do_update_D:\n","                optD.zero_grad(set_to_none=True)\n","\n","                # real 경로\n","                real_in = inst_noise(diffaug(real))\n","                d_real  = netD(real_in)\n","                y_real  = torch.full((B,), real_val, device=device, dtype=d_real.dtype)\n","                loss_real = criterion(d_real, y_real)\n","\n","                # fake 경로\n","                z = torch.randn(B, CFG[\"z_dim\"], 1, 1, device=device)\n","                with torch.no_grad():\n","                    fake = netG(z)\n","                fake_in = inst_noise(diffaug(fake.detach()))\n","                d_fake  = netD(fake_in)\n","                y_fake  = torch.zeros(B, device=device, dtype=d_fake.dtype)\n","                loss_fake = criterion(d_fake, y_fake)\n","\n","                loss_D = loss_real + loss_fake\n","                # -----\n","                if global_step % 16 == 0:  # 빈도 조절\n","                  real_in_r1 = real_in.detach().requires_grad_(True)\n","                  d_real_r1  = netD(real_in_r1).sum()\n","                  grad = torch.autograd.grad(d_real_r1, real_in_r1, create_graph=True)[0]\n","                  r1 = grad.view(B, -1).pow(2).sum(1).mean()\n","                  loss_D = loss_D + (10.0/2.0)*r1  # gamma=10 권장 시작\n","                # -----\n","                loss_D.backward()\n","                torch.nn.utils.clip_grad_norm_(netD.parameters(), 5.0)\n","                optD.step()\n","            else:\n","                # D 건너뛸 때 로깅 변수 안전히 유지\n","                if 'loss_real' not in locals(): loss_real = torch.tensor(0.0, device=device)\n","                if 'loss_fake' not in locals(): loss_fake = torch.tensor(0.0, device=device)\n","                if 'loss_D'   not in locals(): loss_D   = torch.tensor(0.0, device=device)\n","\n","\n","            # --------- Generator 업데이트 -------------\n","            optG.zero_grad(set_to_none=True)\n","            z2 = torch.randn(B, CFG[\"z_dim\"], 1, 1, device=device)\n","            fake2 = netG(z2)\n","            d_fake2 = netD(inst_noise(diffaug(fake2)))\n","            # y_gen   = torch.ones(B, device=device, dtype=d_fake2.dtype)\n","            # 둘 다 real_val(=0.9)로 통일\n","            y_gen  = torch.full((B,), real_val, device=device, dtype=d_fake2.dtype)\n","            loss_G = criterion(d_fake2, y_gen)\n","            loss_G.backward()\n","            torch.nn.utils.clip_grad_norm_(netG.parameters(), 5.0)\n","            optG.step()\n","\n","            # --------- 상태 기반 DiffAug 보정 ----------\n","            # 200 스텝마다 최근 상태를 보고 임시 전환(너무 잦은 전환 방지)\n","            last_da_change_step = -10**9  # 추가\n","\n","            if global_step % 200 == 0:\n","                try:\n","                    d_val = float(loss_D.item())\n","                    g_val = float(loss_G.item())\n","                    lf    = float(loss_fake.item())\n","                    last_da_change_step = adjust_diffaug(\n","                        diffaug, d_val, g_val, lf, global_step,\n","                        last_change_step=last_da_change_step, cool_down=600\n","                    )\n","                    if lf < 0.01:                    # 아주 낮을 때만 강제 strong\n","                        diffaug.set_strength(\"strong\")\n","                    # G가 너무 이기면 완화\n","                    elif (g_val < 1.8 and d_val > 1.5):\n","                        diffaug.set_strength(\"mild\")\n","                    # else: 스테이지 스케줄 유지\n","                except Exception:\n","                    pass\n","\n","\n","            # --------- 로깅 ----------\n","            if global_step % 200 == 0:\n","                print(f\"[Ep {epoch:03d}] step {global_step:06d} | \"\n","                      f\"D:{loss_D.item():.3f} (r{loss_real.item():.3f}/f{loss_fake.item():.3f}) | \"\n","                      f\"G:{loss_G.item():.3f}\")\n","\n","            global_step += 1\n","\n","\n","        # 에폭 종료: 샘플 시각화 저장(Matplotlib)\n","        with torch.no_grad():\n","            netG.eval()\n","            sample = netG(z_fixed)  # [-1,1]\n","        img_path = f\"{SAMPLE_DIR}/epoch_{epoch:03d}.png\"\n","        save_grid_matplotlib(sample, img_path, nrow=8, title=f\"Epoch {epoch}\")\n","        # print(f\"[Viz] saved: {img_path}\")\n","\n","        # 에폭 저장 규칙(0~100: 20ep, 101~500: 5ep)\n","        if should_save_ckpt(epoch, max_epoch_plan=max_epoch_plan):\n","            save_ckpt(epoch, global_step, netG, netD, optG, optD)\n","\n","    # 최초 러닝 종료 안내(100 후 500까지 이어학습 가이드)\n","    print(f\"[Train] finished. epochs={CFG['epochs']}.\")"],"metadata":{"id":"VcZ4P95sHEyv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 실행\n","# train_dcgan(max_epoch_plan=1500)"],"metadata":{"id":"P2rw-LaeHE4h","colab":{"base_uri":"https://localhost:8080/","height":447},"outputId":"0c99d59f-28d2-48cf-a141-a6793e735a7a","executionInfo":{"status":"error","timestamp":1756378227492,"user_tz":-540,"elapsed":551552,"user":{"displayName":"‍손단하[재학 / 경영학전공]","userId":"03748145936175732953"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CKPT] loaded: /content/drive/MyDrive/dcgan//dcgan_portraits128/ckpts/ckpt_e1055_s093895.pt (epoch 1055, step 93895)\n","[Train] start from epoch=1056, steps=93895\n","[Ep 1057] step 094000 | D:1.416 (r0.777/f0.620) | G:0.793\n","[Ep 1059] step 094200 | D:1.393 (r0.825/f0.568) | G:0.759\n","[CKPT] saved: /content/drive/MyDrive/dcgan//dcgan_portraits128/ckpts/ckpt_e1060_s094340.pt\n","[Ep 1061] step 094400 | D:1.443 (r0.732/f0.701) | G:0.754\n","[Ep 1063] step 094600 | D:1.399 (r0.869/f0.530) | G:0.861\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4205794207.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dcgan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epoch_plan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-1057463080.py\u001b[0m in \u001b[0;36mtrain_dcgan\u001b[0;34m(max_epoch_plan)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# 미니배치 루프\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"gtPe75x4AqUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NZ56jxCIAqgj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## D/G Loss 해석 가이드\n"],"metadata":{"id":"bCjrPYeYIfiZ"}},{"cell_type":"markdown","source":["### 1. **이상적 균형 상태**\n","\n","* `loss_D ≈ 0.5 ~ 1.5`\n","* `loss_G ≈ 0.5 ~ 2.0`\n","* **특징**: D가 real/fake를 완벽히 구분하지 못하고, G도 꾸준히 발전 → 적당히 균형 잡힘.\n","* 이미지: 점점 사람 얼굴 구조가 뚜렷해지고, 에폭이 지날수록 눈·코·입이 구분됨.\n","\n","<br>\n","\n","### 2. **D가 너무 강함 (loss\\_D ↓, loss\\_G ↑)**\n","\n","* `loss_D << 0.3`, `loss_G >> 3`\n","* **현상**:\n","\n","  * D가 real/fake를 거의 완벽히 구분 → G가 제대로 학습 못 함\n","  * G 출력: 뭉개지거나 noise 같은 이미지\n","* **조치**:\n","\n","  * D regularization → 라벨 스무딩(이미 적용), **instance noise** 추가\n","  * 학습률 줄이기: `lr_D ↓` or `전체 lr ↓`\n","  * 증강 약화 (DiffAug 너무 강하면 G가 힘들 수 있음)\n","\n","<br>\n","\n","### 3. **G가 너무 강함 (loss\\_D ↑, loss\\_G ↓)**\n","\n","* `loss_D >> 3`, `loss_G << 0.3`\n","* **현상**:\n","\n","  * D가 거의 속음, fake도 real처럼 인식\n","  * G가 모드 붕괴(mode collapse) → 같은 얼굴만 반복 출력\n","* **조치**:\n","\n","  * D capacity 늘리기 (ndf ↑)\n","  * 증강 강도 ↑ (D가 구분할 힘을 얻도록)\n","  * 학습률 조정: `lr_G ↓`\n","\n","<br>\n","\n","### 4. **둘 다 높은 상태 (loss\\_D ≈ 2~~5, loss\\_G ≈ 2~~5)**\n","\n","* **현상**:\n","\n","  * 둘 다 제대로 학습 못 하는 상황 → 발산 위험\n","  * 이미지: 거의 잡음 수준 유지\n","* **조치**:\n","\n","  * learning rate ↓\n","  * batch size ↑ (가능하다면)\n","  * weight init 다시 확인\n","\n","<br>\n","\n","### 5. **둘 다 낮은 상태 (loss\\_D ≈ 0.1~~0.3, loss\\_G ≈ 0.1~~0.3)**\n","\n","* **현상**:\n","\n","  * G와 D 모두 confidence 너무 높음 → 학습 거의 멈춤 (gradient vanish)\n","  * 출력: 항상 같은 퀄리티, 더 개선 안 됨\n","* **조치**:\n","\n","  * lr ↑ 시도\n","  * dropout/instance noise 추가\n","\n","<br>\n","\n","## 📌 모니터링 팁\n","\n","1. **비율 체크**\n","\n","   * D/G 손실이 **항상 한쪽만 너무 유리하지 않은지** 확인 (한쪽이 완전히 낮으면 문제).\n","2. **시각화**\n","\n","   * 손실 로그만 보지 말고 **매 에폭 샘플 이미지** 비교 → 눈·코·입 뭉개짐, 모드 붕괴, 노이즈 여부 직접 확인.\n","3. **학습 후반**\n","\n","   * G/D 손실이 어느 정도 진동하면서 균형을 유지하는 게 가장 건강한 상태.\n","   * GAN은 “둘 다 0에 수렴”하지 않습니다. (그건 오히려 collapse 신호일 수 있어요)\n","\n","<br>\n","\n","✅ 정리\n","\n","* `loss_D`가 너무 낮고 `loss_G`만 높다 → D 약화 필요.\n","* `loss_D`가 높고 `loss_G`만 낮다 → G 약화 필요.\n","* 둘 다 발산/수렴하면 → lr·batch·증강 조정.\n","* 가장 중요한 건 **샘플 이미지 품질**을 같이 보면서 해석하는 것.\n","\n"],"metadata":{"id":"xh_5_LeRIfpx"}}]}